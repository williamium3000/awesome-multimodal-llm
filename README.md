# Awesome-Multimodal-Large-Language-Models
> A curated list of Multimodal Large Language Models with SFT. 

## Table of Contents

- [Awesome-Multimodal-Large-Language-Models](#awesome-multimodal-large-language-models)
  - [Table of Contents](#table-of-contents)
  - [🔥 Supervised Finetuning](#-supervised-finetuning)
    - [Training Set](#training-set)
  - [🔥 Preference Learning](#-preference-learning)
    - [Training Set](#training-set-1)
  - [🔥 Video-LLM](#-video-llm)
    - [Training Set](#training-set-2)
    - [Training Recipe](#training-recipe)
    - [Evaluation Dataset](#evaluation-dataset)
  - [🔥 MM-CoT](#-mm-cot)
    - [Training Dataset](#training-dataset)
    - [Evaluation Dataset](#evaluation-dataset-1)


## 🔥 Supervised Finetuning

<details>

  <summary>Visual Instruction Tuning, NIPS 2023 Oral -> LLaVA-Instruct-150K</summary>

  [paper](https://arxiv.org/abs/2304.08485) | [Github](https://github.com/haotian-liu/LLaVA) | [website](https://llava-vl.github.io/)
  
</details>

<details>

  <summary>Improved Baselines with Visual Instruction Tuning, CVPR 2024 Highlight -> LLaVA-Instruct-665K</summary>

  [paper](https://arxiv.org/abs/2310.03744) | [Github](https://github.com/haotian-liu/LLaVA) | [website](https://llava-vl.github.io/)

</details>

<details>

  <summary>CogVLM: Visual Expert for Pretrained Language Models, 2023 -> CogVLM-SFT-311K</summary>

  [paper](https://arxiv.org/abs/2311.03079) | [Github](https://github.com/THUDM/CogVLM)

</details>

<details>

  <summary>LLaVA-OneVision: Easy Visual Task Transfer, 2024 -> LLaVA-OneVision-Data</summary>

  [paper](https://arxiv.org/abs/2408.03326) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/)

</details>

<details>

  <summary>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions, ECCV 2024 -> ShareGPT4V</summary>
    
  [paper](https://arxiv.org/abs/2311.12793) | [Github](https://github.com/ShareGPT4Omni/ShareGPT4V) | [website](https://sharegpt4v.github.io/)

</details>

<details>

  <summary>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions, NIPS 2024 -> ShareGPT4Video</summary>

  [paper](https://arxiv.org/abs/2406.04325v1) | [Github](https://github.com/ShareGPT4Omni/ShareGPT4Video) | [website](https://sharegpt4video.github.io/)

</details>

<details>

  <summary>Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data, 2024 -> Infinity-MM</summary>

  [paper](https://arxiv.org/abs/2410.18558)

</details>

<details>

  <summary>Video Instruction Tuning with Synthetic Data, 2024 -> LLaVA-Video-178K</summary>

  [paper](https://arxiv.org/abs/2410.02713) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-09-30-llava-video)

</details>

<details>
  
  <summary>LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models, 2024 -> M4-Instruct-Data</summary>

  [paper](https://arxiv.org/abs/2407.07895) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/)

</details>

<details>
  
  <summary>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs, 2024 -> Cambrian-10M</summary>

  [paper](https://arxiv.org/abs/2406.16860) | [Github](https://github.com/cambrian-mllm/cambrian) | [website](https://cambrian-mllm.github.io/) 

</details>

### Training Set

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
| LLaVA-Instruct-150K | LLaVA | Image | 150k |   | [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json)
| LLaVA-Instruct-665K | LLaVA-1.5 | Image | 665k |   | [LLaVA-Instruct-665K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json)
| CogVLM-SFT-311K | CogVLM | Image | 311k | English & Chinese | [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)
| LLaVA-OneVision-Data | LLaVA-OneVision | Image, Video | 1.6M |   | [LLaVA-OneVision-Data](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data)
| ShareGPT4V | ShareGPT4V | Image | 1.2M | | [ShareGPT4V](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V)
| ShareGPT4Video | ShareGPT4Video | Video | 4.8M | | [ShareGPT4Video](https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video)
| Infinity-MM | Aquila-VL | Image | 34.7M | | [Infinity-MM](https://huggingface.co/datasets/Infinity-MM/Infinity-MM)
| LLaVA-Video-178K | LLaVA-OneVision (SI) | Video | 178k | Generated by GPT-4o | [LLaVA-Video-178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K)
| M4-Instruct-Data | LLaVA-NeXT-Interleave | Image, Video | 1177.6K | Generated by GPT-4V | [M4-Instruct-Data](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data)
| InternVL-Chat-V1-2-SFT-Data | InternVL-Chat-V1-2 | Image | 1.2M | | [InternVL-Chat-V1-2](https://huggingface.co/datasets/OpenGVLab/InternVL-Chat-V1-2-SFT-Data)
| Cambrian-10M | Cambrian-1 | Image | 10M | | [Cambrian-10M](https://huggingface.co/datasets/nyu-visionx/Cambrian-10M)



## 🔥 Preference Learning

<details>

  <summary>RLHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback, CVPR 2024 -> RLHF-V-Dataset</summary>

  [paper](https://arxiv.org/abs/2312.00849) | [Github](https://github.com/RLHF-V/RLHF-V) | [website](https://rlhf-v.github.io/)

</details>

<details>

  <summary>RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness, 2024 -> RLAIF-V-Dataset</summary>

  [paper](https://arxiv.org/abs/2405.17220) | [Github](https://github.com/RLHF-V/RLAIF-V)

</details>

<details>

  <summary>Silkie: Preference Distillation for Large Visual Language Models, CoRR 2023 -> VLFeedback</summary>

  [paper](https://arxiv.org/abs/2312.10665) | [Github](https://github.com/vlf-silkie/VLFeedback) | [website](https://vlf-silkie.github.io/)

</details>

<details>
  
  <summary>SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model, 2024 -> SPA-VL</summary>
  
  [paper](https://arxiv.org/abs/2406.12030) | [Github](https://github.com/EchoseChen/SPA-VL-RLHF) | [website](https://sqrti.github.io/SPA-VL/)

</details>

<details>
  
  <summary>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization, 2024 -> MMPR</summary>
  
  [paper](https://arxiv.org/abs/2411.10442) | [Github](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo) | [website](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)

</details>

### Training Set

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
|RLHF-V-Dataset | MiniCPM-V 2.0 | Image | 5.7k | | [RLHF-V-Dataset](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset) |
| RLAIF-V-Dataset | MiniCPM-Llama3-V 2.5 | Image | 83k | | [RLAIF-V-Dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) |
| VLFeedback | Silkie | Image | 380k | | [VLFeedback](https://huggingface.co/datasets/MMInstruction/VLFeedback) |
| SPA-VL | SPA-VL-DP | Image | 100k | Safety | [SPA-VL](https://huggingface.co/datasets/sqrti/SPA-VL) |
| MMPR | InternVL2 | Image | 3M | | [MMPR](https://huggingface.co/datasets/OpenGVLab/MMPR) |



## 🔥 Video-LLM

<details>
<summary>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding, 2023 -> Video-LLaMA</summary>

[paper](https://arxiv.org/pdf/2306.02858) | [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | [website](https://video-llama.github.io/)

**TLDR:** Instruction-tuned audio-visual language model that couples video and audio encoders with LLaMA via projection layers and gated fusion, enabling open-ended video QA, captioning, and dialogue after multi-stage pretraining on web video captions and curated instruction data.
</details>


### Training Set
| Dataset | Source | Data Source | Quantity | Cnstruction Method |
|------------|--------------|--------------|--------------|--------------|
[VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | [Video-ChatGPT](https://arxiv.org/pdf/2306.05424) | ActivityNet | 100k | |

## 🔥 Video-LLM

### Training Recipe
| Method | Training Data | Eval Data | Recipe | Architecture |
|------------|--------------|--------------|--------------|--------------|
| [Video-LLaMA](https://arxiv.org/pdf/2306.02858) | <li> PT video/image-caption: [Webvid-2.5M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md) <li> SFT image-video inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [minigpt-4 3k](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md), [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data)|place_holder | place_holder | place_holder |
| [Video-LLaMA2](https://arxiv.org/pdf/2406.07476) | <li> PT video/image-caption: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), [Panda-70M](https://snap-research.github.io/Panda-70M/), [VIDAL-10M](https://github.com/PKU-YuanGroup/LanguageBind/blob/main/DATASETS.md), [InternVid-10M], CC3M, [DCI](https://github.com/facebookresearch/DCI) <li> SFT image-video inst tuning: [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), In-house data-12k, [Kinetics-710](https://github.com/OpenGVLab/UniFormerV2/blob/main/DATASET.md), [SthSthv2](https://developer.qualcomm.com/software/ai-datasets/something-something), NExTQA, CLEVRER, EgoQA, Tgif, WebVidQA, RealworldQA, Hm3d, Valley, VideoChatGPT, VideoChat, VTimeLLM, VideoChat2, sharegpt4v, [llava-665k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json)| place_holder | place_holder | place_holder |
| [VideoChat](https://arxiv.org/pdf/2305.06355)|<li> PT video/image-caption: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), COCO Cap, VG, SBU, CC3M, CC12M <li> SFT image-video inst tuning: [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), [minigpt-4 3k](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md), 4k from [llava](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file) | place_holder | place_holder | place_holder |
| [VideoChat2](https://arxiv.org/pdf/2305.06355)|<li> Stage1: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), CC3M, CC12M <li> Stage2: COCO, VG, SBU, [InternVid-10M](https://huggingface.co/datasets/OpenGVLab/InternVid) <li> Stage3: [VideoChat2-IT](https://huggingface.co/datasets/OpenGVLab/VideoChat2-IT) | place_holder | place_holder | place_holder |
| [Valley](https://arxiv.org/pdf/2306.07207) | <li> PT video/image-caption: [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)，[Valley-webvid2M-Pretrain-703K ](https://huggingface.co/datasets/luoruipu1/Valley-webvid2M-Pretrain-703K) <li> SFT inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), [Valley-Instruct-65K](https://huggingface.co/datasets/luoruipu1/Valley-Instruct-65k) | place_holder | place_holder | place_holder |
| [Video-ChatGPT](https://arxiv.org/pdf/2306.05424) | [VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | place_holder | place_holder | place_holder |
| [Video-LLaVA](https://arxiv.org/pdf/2311.10122) | <li> PT video/image-caption (same as [Valley](https://arxiv.org/pdf/2306.07207)): [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)，[Valley-webvid2M-Pretrain-703K](https://huggingface.co/datasets/luoruipu1/Valley-webvid2M-Pretrain-703K) <li> SFT inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | [Video-ChatGPT Quantitative Evaluation](https://github.com/mbzuai-oryx/Video-ChatGPT/tree/main/quantitative_evaluation) | place_holder | place_holder |

### Evaluation Dataset

| Dataset | Source | Data Source | Task | Construction Method |
|------------|--------------|--------------|--------------|--------------|
[MVBench](https://github.com/OpenGVLab/Ask-Anything) | [MVBench](https://arxiv.org/abs/2311.17005) | Various | MC-VQA | |

## 🔥 Reasoning

### MM-CoT

#### Training Dataset

| **Name & Link**                                                                                                                  | **Intro**                                                                                                                                                                                              | **Previews**             |
| -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------ |
| [ScienceQA: Learn to Explain (NeurIPS 2022)](https://scienceqa.github.io/)                                                       | 19k multimodal science QA examples (T+I). Teaches CoT reasoning via explanation rationales.<br>**Enrichments:** T-SciQ [55] improves ScienceQA with LLM-generated teaching signals for smaller models. | ![ScienceQA preview 1](./assets/mm-cot/scienceqa-1.png) ![ScienceQA preview 2](./assets/mm-cot/scienceqa-2.png)  |
| [M3CoT (ACL 2024)](https://arxiv.org/pdf/2405.16473)                                                                             | 7.86k multimodal examples with long, 10-step reasoning traces for complex CoT reasoning.                                                                                                               | ![M3CoT preview](./assets/mm-cot/m3cot.png)              |
| [A-OKVQA (A-OKVQAA-OKVQA)](https://huggingface.co/datasets/HuggingFaceM4/A-OKVQA/viewer?views%5B%5D=train&row=0)                 | 25k visual QA tasks requiring world knowledge; integrates textual and visual reasoning.                                                                                                                | ![A-OKVQA preview](./assets/mm-cot/a-okvqa.png)              |
| [Xkev / LLaVA-CoT-100k](https://huggingface.co/datasets/Xkev/LLaVA-CoT-100k)                                                     | 100k multimodal QA samples distilled from LLaVA models with Chain-of-Thought supervision.                                                                                                              |                          |
| VideoCoT [159]                                                                                                                   | Video-based CoT reasoning dataset for temporal and action understanding.                                                                                                                               |                          |
| VideoEspresso                                                                                                                    | Large-scale VideoQA dataset emphasizing temporal multimodal reasoning.                                                                                                                                 |                          |
| MAVIS                                                                                                                            | Math-focused multimodal reasoning dataset bridging visual math and textual explanation.                                                                                                                |                          |
| [ICoT (ICLR 2025)](https://github.com/jungao1106/ICoT)                                                                           | Interleaved-Modal Chain-of-Thought dataset (ICLR 2025), focusing on interleaved visual-text reasoning.                                                                                                 | ![ICoT preview](./assets/mm-cot/icot.png)              |
| [Visual CoT (NeurIPS 2024)](https://huggingface.co/datasets/deepcs233/Visual-CoT)                                                | Comprehensive multimodal CoT benchmark unifying image reasoning, text reasoning, and rationales.                                                                                                       | ![Visual CoT preview 1](./assets/mm-cot/visual-cot-1.png) ![Visual CoT preview 2](./assets/mm-cot/visual-cot-2.png)  |
| [CoMT (AAAI 2025)](https://dl.acm.org/doi/10.1609/aaai.v39i22.34538) | [HF Dataset](https://huggingface.co/datasets/czh-up/CoMT) | “Chain of Multi-modal Thought” benchmark for VQA and multimodal CoT (3.8k samples).                                                                                                                    | ![CoMT preview 1](./assets/mm-cot/comt-1.png) ![CoMT preview 2](./assets/mm-cot/comt-2.png) |
| EMMA-X                                                                                                                           | Augmented robotics reasoning dataset derived from Bridge V2 for embodied multimodal reasoning.                                                                                                         |                          |



#### Evaluation Dataset

| **Name & Link**                                     | **Intro**                                                                                      | **Previews** |
| --------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ------------ |
| [CoMT](https://huggingface.co/datasets/czh-up/CoMT) | 3,853 VQA samples with reasoning chains for common domain multimodal evaluation.               |              |
| OmniBench                                           | 1,142 multimodal (T, I, A) tasks evaluating cross-modal reasoning and CoT explanation quality. |              |
| WorldQA                                             | 1,007 video QA examples (T, V, A) testing open-ended reasoning grounded in world knowledge.    |              |
| MiCEval                                             | 643 open-ended VQA samples evaluating CoT consistency in common domains.                       |              |
| OlympiadBench                                       | 8,476 science QA tasks (Maths & Physics) emphasizing CoT and multi-modal explanation.          |              |
| MME-CoT                                             | 1,130 multimodal (T, I) tasks across science, math, and commonsense domains.                   |              |
| EMMA                                                | 2,788 multimodal QA tasks (T, I) for scientific reasoning with MC & open formats.              |              |
| VisualProcessBench                                  | 2,866 math & science multimodal tasks focused on stepwise process reasoning.                   |              |


##### Rationale Construction (Prompt-based vs Learning-based)
Focuses on how chain-of-thoughts (CoTs) are constructed or taught to the model. Prompt-based: IPVR, VIC, PKRD-CoT, VoT, VideoAgent, LPE, AntGPT. Learning-based: Multimodal-CoT, PCoT, MC-CoT, G-CoT, LoT.

<details>
  <summary>Papers</summary>

  - **IPVR** — Zhenfang Chen et al. “See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-Based Visual Reasoning.” arXiv:2301.05226, 2023.
  - **VIC** — Haojie Zheng et al. “Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination.” arXiv:2411.12591, 2024.
  - **PKRD-CoT** — Xuewen Luo et al. “PKRD-CoT: A Unified Chain-of-Thought Prompting for Multi-Modal Large Language Models in Autonomous Driving.” arXiv:2412.02025, 2024.
  - **VoT** — Hao Fei et al. “Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition.” ICML 2024.
  - **VideoAgent** — Xiaohan Wang et al. “VideoAgent: Long-form Video Understanding with Large Language Model as Agent.” ECCV 2024.
  - **LPE** — Jingran Xie et al. “Leveraging Chain of Thought Towards Empathetic Spoken Dialogue Without Corresponding Question-Answering Data.” arXiv:2501.10937, 2025.
  - **AntGPT** — Qi Zhao et al. “AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?” arXiv:2307.16368, 2023.
  - **Multimodal-CoT** — Yao Yao et al. “Multimodal Chain-of-Thought Reasoning in Language Models.” arXiv:2302.00923, 2023.
  - **PCoT** — Lei Wang et al. “T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering.” AAAI 2024.
  - **MC-CoT** — Cheng Tan et al. “Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-consistency Training.” ECCV 2024.
  - **G-CoT** — Yao Yao, Zuchao Li, and Hai Zhao. “Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models.” arXiv:2305.16582, 2023.
  - **LoT** — Shanshan Zhong et al. “Let’s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation.” CVPR 2024.
</details>

##### Procedural Reasoning (Structured or Autonomous Staging)
Designs explicit multi-stage reasoning flows—either predefined (structured) or adaptive (autonomous). Defined (structured) staging: BDoG, Det-CoT, VisualSketchpad, CoTDet, CoCoT, SegPref. Autonomous staging: PS-CoT, DDCoT, AVQA-CoT, CoT-PT, Image-of-Thought.

<details>
  <summary>Papers</summary>

  - **BDoG** — Changmeng Zheng et al. “A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning.” ACM MM 2024.
  - **Det-CoT** — Yixuan Wu et al. “DetToolchain: A New Prompting Paradigm to Unleash Detection Ability of MLLM.” ECCV 2024.
  - **VisualSketchpad** — Yushi Hu et al. “Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.” arXiv:2406.09403, 2024.
  - **CoTDet** — Jiajin Tang et al. “CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection.” ICCV 2023.
  - **CoCoT** — Daoan Zhang et al. “CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs.” arXiv:2401.02582, 2024.
  - **SegPref** — Yaoting Wang et al. “Can Textual Semantics Mitigate Sounding Object Segmentation Preference?” ECCV 2024.
  - **PS-CoT** — Qun Li et al. “PS-CoT-Adapter: Adapting Plan-and-Solve Chain-of-Thought for ScienceQA.” Science China Information Sciences 68(1):119101, 2025.
  - **DDCoT** — Ge Zheng et al. “DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models.” NeurIPS 2023.
  - **AVQA-CoT** — Guangyao Li et al. “AVQA-CoT: When CoT Meets Question Answering in Audio-Visual Scenarios.” CVPR Workshops 2024.
  - **CoT-PT** — Jiaxin Ge et al. “Chain of Thought Prompt Tuning in Vision Language Models.” arXiv:2304.07919, 2023.
  - **Image-of-Thought** — Qiji Zhou et al. “Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models.” arXiv:2405.13872, 2024.
</details>

##### Information Enhancement (World / Context Retrieval & Tools)
Uses external knowledge or tool-chains to improve factuality or grounding during reasoning. Expert/tool augmentation: Chain-of-Image, Det-CoT, L3GO. World-knowledge retrieval: RAGAR, AR-MCTS, G-CoT, Chain-of-Action, KAM-CoT. In-context memory/retrieval: MCoT-Memory, MGCoT, CCoT.

<details>
  <summary>Papers</summary>

  - **Chain-of-Image** — Fanxu Meng et al. “Chain of Images for Intuitively Reasoning.” arXiv:2311.09241, 2023.
  - **Det-CoT** — Yixuan Wu et al. “DetToolchain: A New Prompting Paradigm to Unleash Detection Ability of MLLM.” ECCV 2024.
  - **L3GO** — Yutaro Yamada et al. “L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects.” arXiv:2402.09052, 2024.
  - **RAGAR** — M. Abdul Khaliq et al. “RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking Using Multimodal Large Language Models.” arXiv:2404.12065, 2024.
  - **AR-MCTS** — Guanting Dong et al. “Progressive Multimodal Reasoning via Active Retrieval.” arXiv:2412.14835, 2024.
  - **G-CoT** — Yao Yao, Zuchao Li, and Hai Zhao. “Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models.” arXiv:2305.16582, 2023.
  - **Chain-of-Action** — Zhenyu Pan et al. “Chain-of-Action: Faithful and Multimodal Question Answering Through Large Language Models.” arXiv:2403.17359, 2024.
  - **KAM-CoT** — Debjyoti Mondal et al. “KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning.” AAAI 2024.
  - **MCoT-Memory** — Xiwen Liang et al. “Memory-driven Multimodal Chain of Thought for Embodied Long-horizon Task Planning.” OpenReview, 2025.
  - **MGCoT** — Yao Yao, Zuchao Li, and Hai Zhao. “Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models.” arXiv:2305.16582, 2023.
  - **CCoT** — Chancharik Mitra et al. “Compositional Chain-of-Thought Prompting for Large Multimodal Models.” CVPR 2024.
</details>

##### Structural & Attention-Driven Reasoning
Focuses on representation structure, cross-modal attention, or modality-specific alignment to improve reasoning interpretability. Asynchronous modality modeling: Audio-CoT, Grounding-Prompter, TextCoT, Cantor, VIC. With attention: Meaformer (EMNLP 2023) introduces hierarchical multimodal transformers with neighbor features and entity-type-aware self-attention.

<details>
  <summary>Papers</summary>

  - **Audio-CoT** — Ziyang Ma et al. “Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model.” arXiv:2501.07246, 2025.
  - **Grounding-Prompter** — Houlun Chen et al. “GroundingPrompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos.” arXiv:2312.17117, 2023.
  - **TextCoT** — Bozhi Luan et al. “TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding.” arXiv:2404.09797, 2024.
  - **Cantor** — Timin Gao et al. “Cantor: Inspiring Multimodal Chain-of-Thought of MLLM.” ACM MM 2024.
  - **VIC** — Haojie Zheng et al. “Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination.” arXiv:2411.12591, 2024.
  - **Meaformer** — EMNLP 2023 (hierarchical multimodal transformer with neighbor features and entity-type-aware self-attention).
</details>

##### Training-Time Injection & Visual Conditioning
Modifies the learning or fine-tuning process itself to optimize where and how vision is injected or preserved during long CoT reasoning. Injection during training: TVC (ACL 2025) performs visual conditioning by shifting visual tokens to critical reasoning stages via dynamic pruning. Related approaches include long-CoT curriculum or modality rebalancing methods such as Visual-o1.

<details>
  <summary>Papers</summary>

  - **TVC** — ACL 2025 (Visual conditioning via dynamic pruning; citation forthcoming).
  - **Visual-o1** — Minheng Ni et al. “Visual-o1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-Thoughts Reasoning.” arXiv:2410.03321, 2024.
</details>

##### Optimization & Test-Time Scaling
Improves inference-time reasoning quality or credit assignment through scaling, reinforcement, or meta-optimization. Slow thinking/iterative reasoning: Visual-o1, LlamaV-o1, Virgo. Reinforcement learning and self-refinement: Deepseek-R1, LLaVA-Reasoner.

<details>
  <summary>Papers</summary>

  - **Visual-o1** — Minheng Ni et al. “Visual-o1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-Thoughts Reasoning.” arXiv:2410.03321, 2024.
  - **LlamaV-o1** — Omkar Thawakar et al. “LlamaV-o1: Rethinking Step-by-Step Visual Reasoning in LLMs.” arXiv:2501.06186, 2025.
  - **Virgo** — Yifan Du et al. “Virgo: A Preliminary Exploration on Reproducing o1-like MLLM.” arXiv:2501.01904, 2025.
  - **Deepseek-R1** — Daya Guo et al. “Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.” arXiv:2501.12948, 2025.
  - **LLaVA-Reasoner** — Ruohong Zhang et al. “Improve Vision Language Model Chain-of-Thought Reasoning.” arXiv:2410.16198, 2024.
</details>

#### Reinforcement Learning
