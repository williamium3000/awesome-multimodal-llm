# Awesome-Multimodal-Large-Language-Models
> A curated list of Multimodal Large Language Models with SFT. 

## Table of Contents

- [Awesome-Multimodal-Large-Language-Models](#awesome-multimodal-large-language-models)
  - [Table of Contents](#table-of-contents)
  - [üî• Supervised Finetuning](#-supervised-finetuning)
    - [Training Set](#training-set)
  - [üî• Preference Learning](#-preference-learning)
    - [Training Set](#training-set-1)
  - [üî• Video-LLM](#-video-llm)
    - [Training Set](#training-set-2)
    - [Training Recipe](#training-recipe)
    - [Evaluation Dataset](#evaluation-dataset)
  - [üî• Reasoning](#-reasoning)
    - [MM-CoT](#mm-cot)
    - [Reinforcement Learning](#reinforcement-learning)


## üî• Supervised Finetuning

<details>

  <summary>Visual Instruction Tuning, NIPS 2023 Oral -> LLaVA-Instruct-150K</summary>

  [paper](https://arxiv.org/abs/2304.08485) | [Github](https://github.com/haotian-liu/LLaVA) | [website](https://llava-vl.github.io/)
  
</details>

<details>

  <summary>Improved Baselines with Visual Instruction Tuning, CVPR 2024 Highlight -> LLaVA-Instruct-665K</summary>

  [paper](https://arxiv.org/abs/2310.03744) | [Github](https://github.com/haotian-liu/LLaVA) | [website](https://llava-vl.github.io/)

</details>

<details>

  <summary>CogVLM: Visual Expert for Pretrained Language Models, 2023 -> CogVLM-SFT-311K</summary>

  [paper](https://arxiv.org/abs/2311.03079) | [Github](https://github.com/THUDM/CogVLM)

</details>

<details>

  <summary>LLaVA-OneVision: Easy Visual Task Transfer, 2024 -> LLaVA-OneVision-Data</summary>

  [paper](https://arxiv.org/abs/2408.03326) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/)

</details>

<details>

  <summary>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions, ECCV 2024 -> ShareGPT4V</summary>
    
  [paper](https://arxiv.org/abs/2311.12793) | [Github](https://github.com/ShareGPT4Omni/ShareGPT4V) | [website](https://sharegpt4v.github.io/)

</details>

<details>

  <summary>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions, NIPS 2024 -> ShareGPT4Video</summary>

  [paper](https://arxiv.org/abs/2406.04325v1) | [Github](https://github.com/ShareGPT4Omni/ShareGPT4Video) | [website](https://sharegpt4video.github.io/)

</details>

<details>

  <summary>Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data, 2024 -> Infinity-MM</summary>

  [paper](https://arxiv.org/abs/2410.18558)

</details>

<details>

  <summary>Video Instruction Tuning with Synthetic Data, 2024 -> LLaVA-Video-178K</summary>

  [paper](https://arxiv.org/abs/2410.02713) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-09-30-llava-video)

</details>

<details>
  
  <summary>LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models, 2024 -> M4-Instruct-Data</summary>

  [paper](https://arxiv.org/abs/2407.07895) | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [website](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/)

</details>

<details>
  
  <summary>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs, 2024 -> Cambrian-10M</summary>

  [paper](https://arxiv.org/abs/2406.16860) | [Github](https://github.com/cambrian-mllm/cambrian) | [website](https://cambrian-mllm.github.io/) 

</details>

### Training Set

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
| LLaVA-Instruct-150K | LLaVA | Image | 150k |   | [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json)
| LLaVA-Instruct-665K | LLaVA-1.5 | Image | 665k |   | [LLaVA-Instruct-665K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json)
| CogVLM-SFT-311K | CogVLM | Image | 311k | English & Chinese | [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)
| LLaVA-OneVision-Data | LLaVA-OneVision | Image, Video | 1.6M |   | [LLaVA-OneVision-Data](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data)
| ShareGPT4V | ShareGPT4V | Image | 1.2M | | [ShareGPT4V](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V)
| ShareGPT4Video | ShareGPT4Video | Video | 4.8M | | [ShareGPT4Video](https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video)
| Infinity-MM | Aquila-VL | Image | 34.7M | | [Infinity-MM](https://huggingface.co/datasets/Infinity-MM/Infinity-MM)
| LLaVA-Video-178K | LLaVA-OneVision (SI) | Video | 178k | Generated by GPT-4o | [LLaVA-Video-178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K)
| M4-Instruct-Data | LLaVA-NeXT-Interleave | Image, Video | 1177.6K | Generated by GPT-4V | [M4-Instruct-Data](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data)
| InternVL-Chat-V1-2-SFT-Data | InternVL-Chat-V1-2 | Image | 1.2M | | [InternVL-Chat-V1-2](https://huggingface.co/datasets/OpenGVLab/InternVL-Chat-V1-2-SFT-Data)
| Cambrian-10M | Cambrian-1 | Image | 10M | | [Cambrian-10M](https://huggingface.co/datasets/nyu-visionx/Cambrian-10M)



## üî• Preference Learning

<details>

  <summary>RLHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback, CVPR 2024 -> RLHF-V-Dataset</summary>

  [paper](https://arxiv.org/abs/2312.00849) | [Github](https://github.com/RLHF-V/RLHF-V) | [website](https://rlhf-v.github.io/)

</details>

<details>

  <summary>RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness, 2024 -> RLAIF-V-Dataset</summary>

  [paper](https://arxiv.org/abs/2405.17220) | [Github](https://github.com/RLHF-V/RLAIF-V)

</details>

<details>

  <summary>Silkie: Preference Distillation for Large Visual Language Models, CoRR 2023 -> VLFeedback</summary>

  [paper](https://arxiv.org/abs/2312.10665) | [Github](https://github.com/vlf-silkie/VLFeedback) | [website](https://vlf-silkie.github.io/)

</details>

<details>
  
  <summary>SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model, 2024 -> SPA-VL</summary>
  
  [paper](https://arxiv.org/abs/2406.12030) | [Github](https://github.com/EchoseChen/SPA-VL-RLHF) | [website](https://sqrti.github.io/SPA-VL/)

</details>

<details>
  
  <summary>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization, 2024 -> MMPR</summary>
  
  [paper](https://arxiv.org/abs/2411.10442) | [Github](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo) | [website](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)

</details>

### Training Set

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
|RLHF-V-Dataset | MiniCPM-V 2.0 | Image | 5.7k | | [RLHF-V-Dataset](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset) |
| RLAIF-V-Dataset | MiniCPM-Llama3-V 2.5 | Image | 83k | | [RLAIF-V-Dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) |
| VLFeedback | Silkie | Image | 380k | | [VLFeedback](https://huggingface.co/datasets/MMInstruction/VLFeedback) |
| SPA-VL | SPA-VL-DP | Image | 100k | Safety | [SPA-VL](https://huggingface.co/datasets/sqrti/SPA-VL) |
| MMPR | InternVL2 | Image | 3M | | [MMPR](https://huggingface.co/datasets/OpenGVLab/MMPR) |



## üî• Video-LLM

<details>
<summary>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding, 2023 -> Video-LLaMA</summary>

[paper](https://arxiv.org/pdf/2306.02858) | [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | [website](https://video-llama.github.io/)

**TLDR:** Instruction-tuned audio-visual language model that couples video and audio encoders with LLaMA via projection layers and gated fusion, enabling open-ended video QA, captioning, and dialogue after multi-stage pretraining on web video captions and curated instruction data.
</details>


### Training Set
| Dataset | Source | Data Source | Quantity | Cnstruction Method |
|------------|--------------|--------------|--------------|--------------|
[VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | [Video-ChatGPT](https://arxiv.org/pdf/2306.05424) | ActivityNet | 100k | |

## üî• Video-LLM

### Training Recipe
| Method | Training Data | Eval Data | Recipe | Architecture |
|------------|--------------|--------------|--------------|--------------|
| [Video-LLaMA](https://arxiv.org/pdf/2306.02858) | <li> PT video/image-caption: [Webvid-2.5M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md) <li> SFT image-video inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [minigpt-4 3k](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md), [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data)|place_holder | place_holder | place_holder |
| [Video-LLaMA2](https://arxiv.org/pdf/2406.07476) | <li> PT video/image-caption: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), [Panda-70M](https://snap-research.github.io/Panda-70M/), [VIDAL-10M](https://github.com/PKU-YuanGroup/LanguageBind/blob/main/DATASETS.md), [InternVid-10M], CC3M, [DCI](https://github.com/facebookresearch/DCI) <li> SFT image-video inst tuning: [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), In-house data-12k, [Kinetics-710](https://github.com/OpenGVLab/UniFormerV2/blob/main/DATASET.md), [SthSthv2](https://developer.qualcomm.com/software/ai-datasets/something-something), NExTQA, CLEVRER, EgoQA, Tgif, WebVidQA, RealworldQA, Hm3d, Valley, VideoChatGPT, VideoChat, VTimeLLM, VideoChat2, sharegpt4v, [llava-665k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json)| place_holder | place_holder | place_holder |
| [VideoChat](https://arxiv.org/pdf/2305.06355)|<li> PT video/image-caption: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), COCO Cap, VG, SBU, CC3M, CC12M <li> SFT image-video inst tuning: [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), [minigpt-4 3k](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md), 4k from [llava](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file) | place_holder | place_holder | place_holder |
| [VideoChat2](https://arxiv.org/pdf/2305.06355)|<li> Stage1: [Webvid-10M](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/), CC3M, CC12M <li> Stage2: COCO, VG, SBU, [InternVid-10M](https://huggingface.co/datasets/OpenGVLab/InternVid) <li> Stage3: [VideoChat2-IT](https://huggingface.co/datasets/OpenGVLab/VideoChat2-IT) | place_holder | place_holder | place_holder |
| [Valley](https://arxiv.org/pdf/2306.07207) | <li> PT video/image-caption: [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)Ôºå[Valley-webvid2M-Pretrain-703K ](https://huggingface.co/datasets/luoruipu1/Valley-webvid2M-Pretrain-703K) <li> SFT inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [videochat-11k](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data), [Valley-Instruct-65K](https://huggingface.co/datasets/luoruipu1/Valley-Instruct-65k) | place_holder | place_holder | place_holder |
| [Video-ChatGPT](https://arxiv.org/pdf/2306.05424) | [VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | place_holder | place_holder | place_holder |
| [Video-LLaVA](https://arxiv.org/pdf/2311.10122) | <li> PT video/image-caption (same as [Valley](https://arxiv.org/pdf/2306.07207)): [LLaVA-CC3M](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)Ôºå[Valley-webvid2M-Pretrain-703K](https://huggingface.co/datasets/luoruipu1/Valley-webvid2M-Pretrain-703K) <li> SFT inst tuning: [llava-150k](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [VideoInstruct100K](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/docs/train_video_chatgpt.md) | [Video-ChatGPT Quantitative Evaluation](https://github.com/mbzuai-oryx/Video-ChatGPT/tree/main/quantitative_evaluation) | place_holder | place_holder |

### Evaluation Dataset

| Dataset | Source | Data Source | Task | Construction Method |
|------------|--------------|--------------|--------------|--------------|
[MVBench](https://github.com/OpenGVLab/Ask-Anything) | [MVBench](https://arxiv.org/abs/2311.17005) | Various | MC-VQA | |

## üî• Reasoning

### MM-CoT

#### Training Dataset

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
| ScienceQA (NeurIPS 2022) | ScienceQA | Text + Image | 19k | Teaches CoT reasoning via detailed explanations; T-SciQ enriches prompts for smaller models.<br>![ScienceQA preview 1](./assets/mm-cot/scienceqa-1.png) ![ScienceQA preview 2](./assets/mm-cot/scienceqa-2.png) | [Website](https://scienceqa.github.io/) |
| M3CoT (ACL 2024) | M3CoT | Text + Image | 7.86k | Long, 10-step multimodal reasoning traces curated for complex CoT supervision.<br>![M3CoT preview](./assets/mm-cot/m3cot.png) | [Paper](https://arxiv.org/abs/2405.16473) |
| A-OKVQA | A-OKVQA | Text + Image | 25k | Visual QA tasks grounded in world knowledge with CoT-style rationales.<br>![A-OKVQA preview](./assets/mm-cot/a-okvqa.png) | [Dataset](https://huggingface.co/datasets/HuggingFaceM4/A-OKVQA) |
| LLaVA-CoT-100k | LLaVA / X-KEV | Text + Image | 100k | CoT-distilled multimodal QA samples derived from LLaVA to supervise reasoning. | [Dataset](https://huggingface.co/datasets/Xkev/LLaVA-CoT-100k) |
| VideoCoT | VideoCoT | Text + Video | ‚Äî | Video-based CoT corpus with active annotation for temporal reasoning. | [Paper](https://arxiv.org/abs/2407.05355) |
| VideoEspresso | VideoEspresso | Text + Video | ‚Äî | Large-scale video QA dataset emphasizing fine-grained temporal CoT reasoning. | [Paper](https://arxiv.org/abs/2411.14794) |
| MAVIS | MAVIS | Text + Image | ‚Äî | Math-focused multimodal reasoning dataset aligning visual math with textual explanations. | ‚Äî |
| ICoT (ICLR 2025) | ICoT | Text + Image (Interleaved) | ‚Äî | Interleaved visual-text chain-of-thought data for structured CoT supervision.<br>![ICoT preview](./assets/mm-cot/icot.png) | [Project](https://github.com/jungao1106/ICoT) |
| Visual-CoT (NeurIPS 2024) | Visual-CoT | Text + Image | ‚Äî | Unified benchmark combining image reasoning, text reasoning, and generated rationales.<br>![Visual CoT preview 1](./assets/mm-cot/visual-cot-1.png) ![Visual CoT preview 2](./assets/mm-cot/visual-cot-2.png) | [Dataset](https://huggingface.co/datasets/deepcs233/Visual-CoT) |
| CoMT (AAAI 2025) | CoMT | Text + Image | 3.8k | Chain-of-multi-modal-thought benchmark for VQA with visual previews.<br>![CoMT preview 1](./assets/mm-cot/comt-1.png) ![CoMT preview 2](./assets/mm-cot/comt-2.png) | [Paper](https://dl.acm.org/doi/10.1609/aaai.v39i22.34538) ¬∑ [Dataset](https://huggingface.co/datasets/czh-up/CoMT) |
| EMMA-X | EMMA-X | Text + Image + Action | ‚Äî | Robotics-oriented multimodal reasoning data derived from Bridge V2 for embodied CoT. | ‚Äî |


#### Evaluation Dataset

| Dataset | Model | Modality | Quantity | Notes | Link |
|---------|-------|----------|----------|-------|------|
| CoMT Eval | CoMT | Text + Image | 3,853 | Common-domain multimodal evaluation split with reasoning traces. | [Dataset](https://huggingface.co/datasets/czh-up/CoMT) |
| OmniBench | OmniBench | Text + Image + Audio | 1,142 | Evaluates cross-modal reasoning and CoT explanation quality. | ‚Äî |
| WorldQA | WorldQA | Text + Video + Audio | 1,007 | Video-centric QA testing world knowledge grounding. | ‚Äî |
| MiCEval | MiCEval | Text + Image | 643 | Open-ended VQA samples measuring CoT consistency. | ‚Äî |
| OlympiadBench | OlympiadBench | Text + Image | 8,476 | Science QA benchmark focusing on CoT-rich multimodal explanations. | ‚Äî |
| MME-CoT | MME-CoT | Text + Image | 1,130 | Multimodal CoT tasks across science, math, and commonsense. | ‚Äî |
| EMMA | EMMA | Text + Image | 2,788 | Scientific multimodal QA with mix of MC and open responses. | ‚Äî |
| VisualProcessBench | VisualProcessBench | Text + Image | 2,866 | Math and science process reasoning with stepwise evaluation criteria. | ‚Äî |

#### Methods

##### Prompt-based
Focuses on staged prompting, retrieval-augmented reasoning, and agentic workflows that restructure inference without additional gradient updates.

<details>
  <summary>**AntGPT** ‚Äî Qi Zhao et al., ‚Äú[AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://arxiv.org/abs/2307.16368)‚Äù (2023).</summary>

  [paper](https://arxiv.org/abs/2307.16368) ‚Äî AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?

</details>

<details>
  <summary>**AR-MCTS** ‚Äî Guanting Dong et al., ‚Äú[Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/abs/2412.14835)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2412.14835) ‚Äî Progressive Multimodal Reasoning via Active Retrieval.

</details>

<details>
  <summary>**BDoG** ‚Äî Changmeng Zheng et al., ‚ÄúA Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning‚Äù (ACM MM 2024).</summary>

  A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning.

</details>

<details>
  <summary>**Chain-of-Action** ‚Äî Zhenyu Pan et al., ‚Äú[Chain-of-Action: Faithful and Multimodal Question Answering Through Large Language Models](https://arxiv.org/abs/2403.17359)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2403.17359) ‚Äî Chain-of-Action: Faithful and Multimodal Question Answering Through Large Language Models.

</details>

<details>
  <summary>**Chain-of-Image** ‚Äî Fanxu Meng et al., ‚Äú[Chain of Images for Intuitively Reasoning](https://arxiv.org/abs/2311.09241)‚Äù (2023).</summary>

  [paper](https://arxiv.org/abs/2311.09241) ‚Äî Chain of Images for Intuitively Reasoning.

</details>

<details>
  <summary>**CCoT** ‚Äî Chancharik Mitra et al., ‚ÄúCompositional Chain-of-Thought Prompting for Large Multimodal Models‚Äù (CVPR 2023).</summary>

  Compositional Chain-of-Thought Prompting for Large Multimodal Models.

</details>

<details>
  <summary>**CoCoT** ‚Äî Daoan Zhang et al., ‚Äú[CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs](https://arxiv.org/abs/2401.02582)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2401.02582) ‚Äî CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs.

</details>

<details>
  <summary>**CoTDet** ‚Äî Jiajin Tang et al., ‚ÄúCoTDet: Affordance Knowledge Prompting for Task Driven Object Detection‚Äù (ICCV 2023).</summary>

  CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection.

</details>

<details>
  <summary>**DDCoT** ‚Äî Ge Zheng et al., ‚ÄúDDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models‚Äù (NeurIPS 2023).</summary>

  DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models.

</details>

<details>
  <summary>**Det-CoT** ‚Äî Yixuan Wu et al., ‚ÄúDetToolchain: A New Prompting Paradigm to Unleash Detection Ability of MLLM‚Äù (ECCV 2024).</summary>

  DetToolchain: A New Prompting Paradigm to Unleash Detection Ability of MLLM.

</details>

<details>
  <summary>**Image-of-Thought** ‚Äî Qiji Zhou et al., ‚Äú[Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2405.13872)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2405.13872) ‚Äî Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models.

</details>

<details>
  <summary>**Integrating Vision and Olfaction** ‚Äî Sunzid Hassan et al., ‚ÄúIntegrating Vision and Olfaction via Multi-Modal LLM for Robotic Odor Source Localization‚Äù (Italian National Conference on Sensors 2024).</summary>

  Integrating Vision and Olfaction via Multi-Modal LLM for Robotic Odor Source Localization (Italian National Conference on Sensors 2024).

</details>

<details>
  <summary>**IPVR** ‚Äî Zhenfang Chen et al., ‚Äú[See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-Based Visual Reasoning](https://arxiv.org/abs/2301.05226)‚Äù (2023).</summary>

  [paper](https://arxiv.org/abs/2301.05226) ‚Äî See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-Based Visual Reasoning.

</details>

<details>
  <summary>**KAM-CoT** ‚Äî Debjyoti Mondal et al., ‚ÄúKAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning‚Äù (AAAI 2024).</summary>

  KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning.

</details>

<details>
  <summary>**Language Is Not All You Need** ‚Äî Shaohan Huang et al., ‚ÄúLanguage Is Not All You Need: Aligning Perception with Language Models‚Äù (NeurIPS 2023).</summary>

  Language Is Not All You Need: Aligning Perception with Language Models.

</details>

<details>
  <summary>**L3GO** ‚Äî Yutaro Yamada et al., ‚Äú[L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects](https://arxiv.org/abs/2402.09052)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2402.09052) ‚Äî L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects.

</details>

<details>
  <summary>**LPE** ‚Äî Jingran Xie et al., ‚Äú[Leveraging Chain of Thought Towards Empathetic Spoken Dialogue Without Corresponding Question-Answering Data](https://arxiv.org/abs/2501.10937)‚Äù (2025).</summary>

  [paper](https://arxiv.org/abs/2501.10937) ‚Äî Leveraging Chain of Thought Towards Empathetic Spoken Dialogue Without Corresponding Question-Answering Data.

</details>

<details>
  <summary>**PKRD-CoT** ‚Äî Xuewen Luo et al., ‚Äú[PKRD-CoT: A Unified Chain-of-Thought Prompting for Multi-Modal Large Language Models in Autonomous Driving](https://arxiv.org/abs/2412.02025)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2412.02025) ‚Äî PKRD-CoT: A Unified Chain-of-Thought Prompting for Multi-Modal Large Language Models in Autonomous Driving.

</details>

<details>
  <summary>**RAGAR** ‚Äî M. Abdul Khaliq et al., ‚Äú[RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking Using Multimodal Large Language Models](https://arxiv.org/abs/2404.12065)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2404.12065) ‚Äî RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking Using Multimodal Large Language Models.

</details>

<details>
  <summary>**SegPref** ‚Äî Yaoting Wang et al., ‚ÄúCan Textual Semantics Mitigate Sounding Object Segmentation Preference?‚Äù (ECCV 2024).</summary>

  Can Textual Semantics Mitigate Sounding Object Segmentation Preference?

</details>

<details>
  <summary>**TextCoT** ‚Äî Bozhi Luan et al., ‚Äú[TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding](https://arxiv.org/abs/2404.09797)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2404.09797) ‚Äî TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding.

</details>

<details>
  <summary>**VideoAgent** ‚Äî Xiaohan Wang et al., ‚Äú[VideoAgent: Long-form Video Understanding with Large Language Model as Agent](https://arxiv.org/abs/2403.10517)‚Äù (ECCV 2024).</summary>

  [paper](https://arxiv.org/abs/2403.10517) ‚Äî VideoAgent: Long-form Video Understanding with Large Language Model as Agent.

</details>

<details>
  <summary>**VisualSketchpad** ‚Äî Yushi Hu et al., ‚Äú[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models](https://arxiv.org/abs/2406.09403)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2406.09403) ‚Äî Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.

</details>

<details>
  <summary>**Human I/O** ‚Äî Xingyu Bruce Liu et al., ‚ÄúHuman I/O: Towards a Unified Approach to Detecting Situational Impairments‚Äù (CHI 2024).</summary>

  Human I/O: Towards a Unified Approach to Detecting Situational Impairments.

</details>

<details>
  <summary>**VoT** ‚Äî Hao Fei et al., ‚ÄúVideo-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition‚Äù (ICML 2024).</summary>

  Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition.

</details>

<details>
  <summary>**VIC** ‚Äî Haojie Zheng et al., ‚Äú[Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination](https://arxiv.org/abs/2411.12591)‚Äù (2024).</summary>

  [paper](https://arxiv.org/abs/2411.12591) ‚Äî Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination.

</details>

##### Training-based
Captures methods that adapt model parameters, inject new supervision, or modify architectures to strengthen multimodal chain-of-thought abilities.

<details>
  <summary>Papers</summary>

  - **AVQA-CoT** ‚Äî Guangyao Li et al., ‚ÄúAVQA-CoT: When CoT Meets Question Answering in Audio-Visual Scenarios‚Äù (CVPR Workshops 2024).
  - **Audio-CoT** ‚Äî Ziyang Ma et al., ‚Äú[Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model](https://arxiv.org/abs/2501.07246)‚Äù (2025).
  - **Cantor** ‚Äî Timin Gao et al., ‚ÄúCantor: Inspiring Multimodal Chain-of-Thought of MLLM‚Äù (ACM MM 2024).
  - **CoT-PT** ‚Äî Jiaxin Ge et al., ‚Äú[Chain of Thought Prompt Tuning in Vision Language Models](https://arxiv.org/abs/2304.07919)‚Äù (2023).
  - **G-CoT** ‚Äî Yao Yao, Zuchao Li, and Hai Zhao, ‚Äú[Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2305.16582)‚Äù (2023).
  - **Grounding-Prompter** ‚Äî Houlun Chen et al., ‚Äú[GroundingPrompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos](https://arxiv.org/abs/2312.17117)‚Äù (2023).
  - **LoT** ‚Äî Shanshan Zhong et al., ‚ÄúLet‚Äôs Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation‚Äù (CVPR 2024).
  - **LlamaV-o1** ‚Äî Omkar Thawakar et al., ‚Äú[LlamaV-o1: Rethinking Step-by-Step Visual Reasoning in LLMs](https://arxiv.org/abs/2501.06186)‚Äù (2025).
  - **MC-CoT** ‚Äî Cheng Tan et al., ‚ÄúBoosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-consistency Training‚Äù (ECCV 2024).
  - **MCoT-Memory** ‚Äî Xiwen Liang et al., ‚ÄúMemory-driven Multimodal Chain of Thought for Embodied Long-horizon Task Planning‚Äù (OpenReview 2025).
  - **Meaformer** ‚Äî EMNLP 2023 (introduces neighbor-aware hierarchical multimodal transformers for alignment).
  - **MGCoT** ‚Äî Yao Yao, Zuchao Li, and Hai Zhao, ‚Äú[Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2305.16582)‚Äù (2023).
  - **Multimodal-CoT** ‚Äî Yao Yao et al., ‚Äú[Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)‚Äù (2023).
  - **PCoT** ‚Äî Lei Wang et al., ‚ÄúT-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering‚Äù (AAAI 2024).
  - **PS-CoT** ‚Äî Qun Li et al., ‚ÄúPS-CoT-Adapter: Adapting Plan-and-Solve Chain-of-Thought for ScienceQA‚Äù (Science China Information Sciences, 2025).
  - **TVC** ‚Äî ACL 2025 (Visual conditioning via dynamic pruning; citation forthcoming).
  - **Visual-o1** ‚Äî Minheng Ni et al., ‚Äú[Visual-o1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-Thoughts Reasoning](https://arxiv.org/abs/2410.03321)‚Äù (2024).
  - **Virgo** ‚Äî Yifan Du et al., ‚Äú[Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904)‚Äù (2025).
</details>

### Reinforcement Learning

Focuses on reinforcement learning or policy optimization steps that explicitly reward improved multimodal reasoning behavior.

<details>
  <summary>Papers</summary>

  - **Deepseek-R1** ‚Äî Daya Guo et al., ‚Äú[Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)‚Äù (2025).
  - **LLaVA-Reasoner** ‚Äî Ruohong Zhang et al., ‚Äú[Improve Vision Language Model Chain-of-Thought Reasoning](https://arxiv.org/abs/2410.16198)‚Äù (2024).
</details>
